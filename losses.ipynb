{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CrossEntropy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from src.losses import CrossEntropyLoss\n",
                "from src.util import Logger\n",
                "import numpy as np\n",
                "\n",
                "device = 2\n",
                "logger = Logger(1)\n",
                "\n",
                "loss_fn_ce = CrossEntropyLoss(device=device, logger=logger)\n",
                "loss_fn_ce_aug = CrossEntropyLoss(device=device, logger=logger, has_aug_ax=True)\n",
                "\n",
                "B, K, C, W, H = 16, 8, 3, 224, 224\n",
                "out_D = 20\n",
                "mock_out_ce = {\"logits\": torch.Tensor(B, out_D)}\n",
                "mock_batch_ce = {\n",
                "    \"lbl\": torch.Tensor(np.random.randint(0, out_D, B)).to(int),\n",
                "    \"img\": torch.Tensor(B, C, W, H),\n",
                "}\n",
                "mock_out_ce_aug = {\"logits\": torch.Tensor(B, K, out_D)}\n",
                "mock_batch_ce_aug = {\n",
                "    \"lbl\": torch.Tensor(np.random.randint(0, out_D, (B, K))).to(int),\n",
                "    \"img\": torch.Tensor(B, K, C, W, H),\n",
                "}\n",
                "\n",
                "loss_ce = loss_fn_ce(mock_out_ce, mock_batch_ce, plot=False)\n",
                "loss_ce_aug = loss_fn_ce_aug(mock_out_ce_aug, mock_batch_ce_aug, plot=False)\n",
                "\n",
                "assert loss_ce.shape == torch.Size([])\n",
                "assert loss_ce_aug.shape == torch.Size([])\n",
                "assert loss_ce.dtype == torch.float\n",
                "assert loss_ce_aug.dtype == torch.float\n",
                "assert loss_ce.cpu().detach().item() >= 0 or torch.isnan(loss_ce)\n",
                "assert loss_ce_aug.cpu().detach().item() >= 0 or torch.isnan(loss_ce_aug)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MMCR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from src.losses import MMCRLoss\n",
                "from src.util import Logger\n",
                "\n",
                "device = 2\n",
                "logger = Logger(1)\n",
                "\n",
                "loss_fn = MMCRLoss(logger=logger, device=device, lamb=0.01)\n",
                "mock_batch = {\"lbl\": torch.Tensor(16, 8), \"img\": torch.Tensor(16, 8, 3, 224, 224)}\n",
                "mock_out = {\"embs\": torch.Tensor(16, 8, 768)}\n",
                "loss = loss_fn(mock_out, mock_batch, plot=False)\n",
                "\n",
                "assert loss.shape == torch.Size([])\n",
                "assert loss.dtype == torch.float"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ConcatLoss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from omegaconf import OmegaConf\n",
                "from src.util import Logger\n",
                "import torch\n",
                "import numpy as np\n",
                "from src.losses import ConcatLoss, MMCRLoss, CrossEntropyLoss\n",
                "\n",
                "device = 2\n",
                "logger = Logger(1)\n",
                "\n",
                "loss_fn_aug = MMCRLoss(logger=logger, device=device, lamb=0.01)\n",
                "loss_fn_ce = CrossEntropyLoss(device=device, logger=logger, has_aug_ax=True)\n",
                "\n",
                "conf1 = OmegaConf.create(\n",
                "    {\n",
                "        \"aug\": {\"target\": \"src.losses.MMCRLoss\", \"params\": {\"lamb\": 0.01}},\n",
                "        \"cross_entropy\": {\n",
                "            \"target\": \"src.losses.CrossEntropyLoss\",\n",
                "            \"params\": {\"has_aug_ax\": True},\n",
                "        },\n",
                "    }\n",
                ")\n",
                "conf2 = {\n",
                "    \"aug\": loss_fn_aug,\n",
                "    \"cross_entropy\": loss_fn_ce,\n",
                "}\n",
                "\n",
                "for conf in [conf1, conf2]:\n",
                "    conc_loss_fn = ConcatLoss(device, logger, conf)\n",
                "\n",
                "    B, K, C, W, H = 16, 8, 3, 224, 224\n",
                "    emb_D, out_D = 768, 20\n",
                "    mock_batch = {\n",
                "        \"lbl\": torch.Tensor(np.random.randint(0, out_D, (B, K))).to(int),\n",
                "        \"img\": torch.Tensor(B, K, C, H, W),\n",
                "    }\n",
                "    mock_out = {\"embs\": torch.Tensor(B, K, emb_D), \"logits\": torch.Tensor(B, K, out_D)}\n",
                "    loss = conc_loss_fn(mock_out, mock_batch, False)\n",
                "    assert loss.dtype == torch.float\n",
                "    assert loss.shape == torch.Size([])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tearetina",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
